{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate_Model\n",
    "Evaluates our proposed final model on all three test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.utils import Sequence # for data generator class\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to the Datasets and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = os.path.join('I:\\\\','Beleg','data', 'test')\n",
    "path_cloudy = os.path.join(path_base, 'Cloudy')\n",
    "path_night = os.path.join(path_base, 'Night')\n",
    "path_day = os.path.join(path_base, 'Day')\n",
    "path_val = os.path.join('I:\\\\', 'Beleg', 'data', 'validation')\n",
    "path_model = os.path.join('I:\\\\', 'Beleg', 'models_second_tests', 'model_sgd_100_MMAE_simple.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics necessary for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masked_Mean_Absolute_Error(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Sigmoid(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Simple(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Simple_Sigmoid(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Sigmoid(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Simple(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Simple_Sigmoid(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def berHu(c):\n",
    "    '''Reverse Huber loss as stated in paper \"Deeper Depth Prediction with Fully Convolutional Residual Networks\" by Laina et al. and \"The berhu\n",
    "       penalty and the grouped effect\" by L. Zwald and S. Lambert-Lacroix'''\n",
    "    # does this current implementation makes sense? --> yes, it returns mae or mse\n",
    "    # TODO implement this with binary mask too?\n",
    "    def inverse_huber(y_true, y_pred):\n",
    "        threshold = c * K.max(K.abs(y_true - y_pred))\n",
    "        absolute_mean = K.mean(K.abs(y_true - y_pred))\n",
    "        mask = K.less_equal(absolute_mean, threshold)\n",
    "        mask = K.cast(mask, dtype='float32')\n",
    "        return mask * absolute_mean + (1-mask) * K.mean(K.square(K.abs(y_true - y_pred)))\n",
    "    return inverse_huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    '''Assumes that examples in the provided folder are named from 1 to n, with n being the number of images'''\n",
    "    def __init__(self, path_to_data_set='data/train', batch_size=32, image_size=(480,640), shuffle=True):\n",
    "        self.path_to_data = path_to_data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.training_size = self.__get_training_data_size(self.path_to_data)\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        \n",
    "    def __get_training_data_size(self, path_to_data):\n",
    "        '''gets the number of samples'''\n",
    "        path_color = os.path.join(path_to_data,'Color')\n",
    "        if os.path.isdir(path_color):\n",
    "            size = len([color for color in os.listdir(path_color) if os.path.isfile(os.path.join(path_color, color))])\n",
    "            return size\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''Number of batches per epoche'''\n",
    "        return int(np.floor(self.training_size / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        '''Update indices (and their ordering) after each epoch'''\n",
    "        # image names start with 1, np.arange(n,m) returns values from n to (m-1)\n",
    "        self.indices = np.arange(1, self.training_size+1)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "            \n",
    "    def __data_generation(self, list_images):\n",
    "        '''Generates data of size <batch_size>''' # X = (batch_size, 480, 640, 1)\n",
    "        X1 = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32) # color images\n",
    "        X2 = np.empty((self.batch_size, *self.image_size), dtype=np.float32) # ir image\n",
    "        y = np.empty((self.batch_size, *self.image_size), dtype=np.uint16)  # depth image\n",
    "        # Generate data\n",
    "        for idx, name in enumerate(list_images):\n",
    "            # load images in arrays\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Color', str(name)+\".jpg\"), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            X1[idx,] = (img/255.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Infrared', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            X2[idx,] = (img/65535.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Depth', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            y[idx,] = img\n",
    "        # reshape ir and depth images\n",
    "        X2 = X2.reshape(self.batch_size, 480, 640, 1)\n",
    "        y = y.reshape(self.batch_size, 480, 640, 1)  \n",
    "        return X1, X2, y\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Generate one batch of data, X1 contains 8-bit RGB images, X2 16-bit infrared images and y corresponding 16-bit depth images'''\n",
    "        # Generate indices of data   \n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X1, X2, y = self.__data_generation(indices)\n",
    "        return ([X1, X2], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Generators for every Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudy_generator = DataGenerator(\n",
    "    path_to_data_set=path_cloudy,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "\n",
    "night_generator = DataGenerator(\n",
    "    path_to_data_set=path_night,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "\n",
    "day_generator = DataGenerator(\n",
    "    path_to_data_set=path_day,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model that should be utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_model, custom_objects={'Masked_Mean_Absolute_Error_Simple': Masked_Mean_Absolute_Error, \n",
    "                                                        'Masked_Mean_Absolute_Error':Masked_Mean_Absolute_Error,\n",
    "                                                        'Masked_Mean_Absolute_Error_Simple':Masked_Mean_Absolute_Error_Simple, \n",
    "                                                        'Masked_Mean_Absolute_Error_Simple_Sigmoid':Masked_Mean_Absolute_Error_Simple_Sigmoid,\n",
    "                                                        'Masked_Mean_Absolute_Error_Sigmoid':Masked_Mean_Absolute_Error_Sigmoid,\n",
    "                                                        'Masked_Root_Mean_Squared_Error':Masked_Root_Mean_Squared_Error,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Simple':Masked_Root_Mean_Squared_Error_Simple,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Simple_Sigmoid':Masked_Root_Mean_Squared_Error_Simple_Sigmoid,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Sigmoid':Masked_Root_Mean_Squared_Error_Sigmoid,\n",
    "                                                        'inverse_huber':berHu(0.2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate every Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 172s 505ms/step\n"
     ]
    }
   ],
   "source": [
    "eval_cloudy = model.evaluate_generator(cloudy_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 159s 487ms/step\n"
     ]
    }
   ],
   "source": [
    "eval_night = model.evaluate_generator(night_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376/376 [==============================] - 186s 494ms/step\n"
     ]
    }
   ],
   "source": [
    "eval_day = model.evaluate_generator(day_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84054190.13269794,\n",
       " 1973.711239453984,\n",
       " 12007711.551686216,\n",
       " 84054190.13269794,\n",
       " 84054675.90615836,\n",
       " 0.00020866744516538708,\n",
       " 1973.711239453984]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_cloudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss',\n",
       " 'mean_absolute_error',\n",
       " 'mean_squared_error',\n",
       " 'Masked_Mean_Absolute_Error',\n",
       " 'Masked_Root_Mean_Squared_Error',\n",
       " 'acc',\n",
       " 'inverse_huber']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[112243827.29447852,\n",
       " 2271.182699753463,\n",
       " 50383378.49961656,\n",
       " 112243827.29447852,\n",
       " 112244299.92944786,\n",
       " 0.0004344504886607502,\n",
       " 2271.182699753463]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78692632.45744681,\n",
       " 2151.612395753252,\n",
       " 16117591.120678192,\n",
       " 78692632.45744681,\n",
       " 78693216.51595744,\n",
       " 0.0003605641029819108,\n",
       " 2151.612395753252]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1059s 7s/step\n"
     ]
    }
   ],
   "source": [
    "gen = DataGenerator(\n",
    "    path_to_data_set=path_val,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "eval_val = model.evaluate_generator(gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[586.2524173736572,\n",
       " 1164.78076171875,\n",
       " 45156765.028125,\n",
       " 67212461.21875,\n",
       " 67212786.43125,\n",
       " 0.0007875671406509354,\n",
       " 1164.78076171875]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict(zip(model.metrics_names, eval_cloudy))\n",
    "path = os.path.join('I:\\\\', 'Beleg', 'test_results')\n",
    "with open(os.path.join(path, 'cloudy.json'), 'w') as file:\n",
    "    json.dump(dic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
