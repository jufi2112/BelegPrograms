{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate_Model\n",
    "Evaluates our proposed final model on all three test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.utils import Sequence # for data generator class\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to the Datasets and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = os.path.join('I:\\\\','Beleg','data', 'test')\n",
    "path_cloudy = os.path.join(path_base, 'Cloudy')\n",
    "path_night = os.path.join(path_base, 'Night')\n",
    "path_day = os.path.join(path_base, 'Day')\n",
    "path_val = os.path.join('I:\\\\', 'Beleg', 'data', 'validation')\n",
    "#path_model = os.path.join('I:\\\\', 'Beleg', 'models_second_tests', 'model_sgd_100_MMAE_simple.h5')\n",
    "path_model = os.path.join('I:\\\\','Beleg','models_second_tests', 'model_sgd_100_MMAE_simple_momentum_nesterov.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics necessary for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masked_Mean_Absolute_Error(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Sigmoid(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Simple(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Simple_Sigmoid(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Sigmoid(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Simple(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Simple_Sigmoid(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def berHu(c):\n",
    "    '''Reverse Huber loss as stated in paper \"Deeper Depth Prediction with Fully Convolutional Residual Networks\" by Laina et al. and \"The berhu\n",
    "       penalty and the grouped effect\" by L. Zwald and S. Lambert-Lacroix'''\n",
    "    # does this current implementation makes sense? --> yes, it returns mae or mse\n",
    "    # TODO implement this with binary mask too?\n",
    "    def inverse_huber(y_true, y_pred):\n",
    "        threshold = c * K.max(K.abs(y_true - y_pred))\n",
    "        absolute_mean = K.mean(K.abs(y_true - y_pred))\n",
    "        mask = K.less_equal(absolute_mean, threshold)\n",
    "        mask = K.cast(mask, dtype='float32')\n",
    "        return mask * absolute_mean + (1-mask) * K.mean(K.square(K.abs(y_true - y_pred)))\n",
    "    return inverse_huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    '''Assumes that examples in the provided folder are named from 1 to n, with n being the number of images'''\n",
    "    def __init__(self, path_to_data_set='data/train', batch_size=32, image_size=(480,640), shuffle=True):\n",
    "        self.path_to_data = path_to_data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.training_size = self.__get_training_data_size(self.path_to_data)\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        \n",
    "    def __get_training_data_size(self, path_to_data):\n",
    "        '''gets the number of samples'''\n",
    "        path_color = os.path.join(path_to_data,'Color')\n",
    "        if os.path.isdir(path_color):\n",
    "            size = len([color for color in os.listdir(path_color) if os.path.isfile(os.path.join(path_color, color))])\n",
    "            return size\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''Number of batches per epoche'''\n",
    "        return int(np.floor(self.training_size / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        '''Update indices (and their ordering) after each epoch'''\n",
    "        # image names start with 1, np.arange(n,m) returns values from n to (m-1)\n",
    "        self.indices = np.arange(1, self.training_size+1)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "            \n",
    "    def __data_generation(self, list_images):\n",
    "        '''Generates data of size <batch_size>''' # X = (batch_size, 480, 640, 1)\n",
    "        X1 = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32) # color images\n",
    "        X2 = np.empty((self.batch_size, *self.image_size), dtype=np.float32) # ir image\n",
    "        y = np.empty((self.batch_size, *self.image_size), dtype=np.uint16)  # depth image\n",
    "        # Generate data\n",
    "        for idx, name in enumerate(list_images):\n",
    "            # load images in arrays\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Color', str(name)+\".jpg\"), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            X1[idx,] = (img/255.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Infrared', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            X2[idx,] = (img/65535.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Depth', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            y[idx,] = img\n",
    "        # reshape ir and depth images\n",
    "        X2 = X2.reshape(self.batch_size, 480, 640, 1)\n",
    "        y = y.reshape(self.batch_size, 480, 640, 1)  \n",
    "        return X1, X2, y\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Generate one batch of data, X1 contains 8-bit RGB images, X2 16-bit infrared images and y corresponding 16-bit depth images'''\n",
    "        # Generate indices of data   \n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X1, X2, y = self.__data_generation(indices)\n",
    "        return ([X1, X2], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Generators for every Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudy_generator = DataGenerator(\n",
    "    path_to_data_set=path_cloudy,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "\n",
    "night_generator = DataGenerator(\n",
    "    path_to_data_set=path_night,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "\n",
    "day_generator = DataGenerator(\n",
    "    path_to_data_set=path_day,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model that should be utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(path_model, custom_objects={'Masked_Mean_Absolute_Error':Masked_Mean_Absolute_Error,\n",
    "                                                        'Masked_Mean_Absolute_Error_Simple':Masked_Mean_Absolute_Error_Simple, \n",
    "                                                        'Masked_Mean_Absolute_Error_Simple_Sigmoid':Masked_Mean_Absolute_Error_Simple_Sigmoid,\n",
    "                                                        'Masked_Mean_Absolute_Error_Sigmoid':Masked_Mean_Absolute_Error_Sigmoid,\n",
    "                                                        'Masked_Root_Mean_Squared_Error':Masked_Root_Mean_Squared_Error,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Simple':Masked_Root_Mean_Squared_Error_Simple,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Simple_Sigmoid':Masked_Root_Mean_Squared_Error_Simple_Sigmoid,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Sigmoid':Masked_Root_Mean_Squared_Error_Sigmoid,\n",
    "                                                        'inverse_huber':berHu(0.2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate every Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 163s 477ms/step\n"
     ]
    }
   ],
   "source": [
    "eval_cloudy = model.evaluate_generator(cloudy_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 155s 476ms/step\n"
     ]
    }
   ],
   "source": [
    "eval_night = model.evaluate_generator(night_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/376 [==============>...............] - ETA: 2:42"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-96a1f6aaf501>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_day\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mday_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    344\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_day = model.evaluate_generator(day_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1589.0273369484282,\n",
       " 2172.4418607023804,\n",
       " 32522272.348973606,\n",
       " 76603407.22580644,\n",
       " 76603883.25659823,\n",
       " 0.00021108975978680001,\n",
       " 2172.4418607023804]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_cloudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss',\n",
       " 'mean_absolute_error',\n",
       " 'mean_squared_error',\n",
       " 'Masked_Mean_Absolute_Error',\n",
       " 'Masked_Root_Mean_Squared_Error',\n",
       " 'acc',\n",
       " 'inverse_huber']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[935.8168528738197,\n",
       " 2650.0444508183955,\n",
       " 118292945.33128834,\n",
       " 132021303.50920245,\n",
       " 132021805.63190185,\n",
       " 0.00045054929057851097,\n",
       " 2650.0444508183955]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 1059s 7s/step\n"
     ]
    }
   ],
   "source": [
    "gen = DataGenerator(\n",
    "    path_to_data_set=path_val,\n",
    "    batch_size=4,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "eval_val = model.evaluate_generator(gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[586.2524173736572,\n",
       " 1164.78076171875,\n",
       " 45156765.028125,\n",
       " 67212461.21875,\n",
       " 67212786.43125,\n",
       " 0.0007875671406509354,\n",
       " 1164.78076171875]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict(zip(model.metrics_names, eval_cloudy))\n",
    "path = os.path.join('I:\\\\', 'Beleg', 'test_results')\n",
    "with open(os.path.join(path, 'cloudy.json'), 'w') as file:\n",
    "    json.dump(dic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
