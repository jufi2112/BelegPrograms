{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate_Model\n",
    "Evaluates our proposed final model on all three test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import keras.backend as K\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import os\n",
    "from keras.utils import Sequence # for data generator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to the Datasets and the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = os.path.join('/media','julien','Transcend','Beleg','data','test')\n",
    "path_cloudy = os.path.join(path_base, 'Cloudy')\n",
    "path_night = os.path.join(path_base, 'Night')\n",
    "path_day = os.path.join(path_base, 'Day')\n",
    "path_model = os.path.join('/media', 'julien', 'Transcend', 'Beleg', 'models_second_tests', 'model_sgd_100_MMAE_simple_momentum_nesterov.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics necessary for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Masked_Mean_Absolute_Error(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Sigmoid(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Simple(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error_Simple_Sigmoid(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Sigmoid(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Simple(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error_Simple_Sigmoid(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # Since we are using a sigmoid activation function, scale the predictions from [0,1] to [0,65535]\n",
    "    y_pred = y_pred * 65535\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            ) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def berHu(c):\n",
    "    '''Reverse Huber loss as stated in paper \"Deeper Depth Prediction with Fully Convolutional Residual Networks\" by Laina et al. and \"The berhu\n",
    "       penalty and the grouped effect\" by L. Zwald and S. Lambert-Lacroix'''\n",
    "    # does this current implementation makes sense? --> yes, it returns mae or mse\n",
    "    # TODO implement this with binary mask too?\n",
    "    def inverse_huber(y_true, y_pred):\n",
    "        threshold = c * K.max(K.abs(y_true - y_pred))\n",
    "        absolute_mean = K.mean(K.abs(y_true - y_pred))\n",
    "        mask = K.less_equal(absolute_mean, threshold)\n",
    "        mask = K.cast(mask, dtype='float32')\n",
    "        return mask * absolute_mean + (1-mask) * K.mean(K.square(K.abs(y_true - y_pred)))\n",
    "    return inverse_huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    '''Assumes that examples in the provided folder are named from 1 to n, with n being the number of images'''\n",
    "    def __init__(self, path_to_data_set='data/train', batch_size=32, image_size=(480,640), shuffle=True):\n",
    "        self.path_to_data = path_to_data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.training_size = self.__get_training_data_size(self.path_to_data)\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        \n",
    "    def __get_training_data_size(self, path_to_data):\n",
    "        '''gets the number of samples'''\n",
    "        path_color = os.path.join(path_to_data,'Color')\n",
    "        if os.path.isdir(path_color):\n",
    "            size = len([color for color in os.listdir(path_color) if os.path.isfile(os.path.join(path_color, color))])\n",
    "            return size\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''Number of batches per epoche'''\n",
    "        return int(np.floor(self.training_size / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        '''Update indices (and their ordering) after each epoch'''\n",
    "        # image names start with 1, np.arange(n,m) returns values from n to (m-1)\n",
    "        self.indices = np.arange(1, self.training_size+1)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "            \n",
    "    def __data_generation(self, list_images):\n",
    "        '''Generates data of size <batch_size>''' # X = (batch_size, 480, 640, 1)\n",
    "        X1 = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32) # color images\n",
    "        X2 = np.empty((self.batch_size, *self.image_size), dtype=np.float32) # ir image\n",
    "        y = np.empty((self.batch_size, *self.image_size), dtype=np.uint16)  # depth image\n",
    "        # Generate data\n",
    "        for idx, name in enumerate(list_images):\n",
    "            # load images in arrays\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Color', str(name)+\".jpg\"), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            X1[idx,] = (img/255.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Infrared', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            X2[idx,] = (img/65535.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Depth', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            y[idx,] = img\n",
    "        # reshape ir and depth images\n",
    "        X2 = X2.reshape(self.batch_size, 480, 640, 1)\n",
    "        y = y.reshape(self.batch_size, 480, 640, 1)  \n",
    "        return X1, X2, y\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Generate one batch of data, X1 contains 8-bit RGB images, X2 16-bit infrared images and y corresponding 16-bit depth images'''\n",
    "        # Generate indices of data   \n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X1, X2, y = self.__data_generation(indices)\n",
    "        return ([X1, X2], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Generators for every Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudy_generator = DataGenerator(\n",
    "    path_to_data_set=path_cloudy,\n",
    "    batch_size=1,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "\n",
    "night_generator = DataGenerator(\n",
    "    path_to_data_set=path_night,\n",
    "    batch_size=1,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)\n",
    "\n",
    "day_generator = DataGenerator(\n",
    "    path_to_data_set=path_day,\n",
    "    batch_size=1,\n",
    "    image_size=(480,640),\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model that should be utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1129 01:01:48.183093 139841146816320 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1129 01:01:48.216765 139841146816320 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1129 01:01:48.251834 139841146816320 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1129 01:01:48.252460 139841146816320 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1129 01:01:48.252978 139841146816320 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1129 01:01:48.323705 139841146816320 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W1129 01:01:53.994368 139841146816320 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(path_model, custom_objects={'Masked_Mean_Absolute_Error_Simple': Masked_Mean_Absolute_Error, \n",
    "                                                        'Masked_Mean_Absolute_Error':Masked_Mean_Absolute_Error,\n",
    "                                                        'Masked_Mean_Absolute_Error_Simple':Masked_Mean_Absolute_Error_Simple, \n",
    "                                                        'Masked_Mean_Absolute_Error_Simple_Sigmoid':Masked_Mean_Absolute_Error_Simple_Sigmoid,\n",
    "                                                        'Masked_Mean_Absolute_Error_Sigmoid':Masked_Mean_Absolute_Error_Sigmoid,\n",
    "                                                        'Masked_Root_Mean_Squared_Error':Masked_Root_Mean_Squared_Error,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Simple':Masked_Root_Mean_Squared_Error_Simple,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Simple_Sigmoid':Masked_Root_Mean_Squared_Error_Simple_Sigmoid,\n",
    "                                                        'Masked_Root_Mean_Squared_Error_Sigmoid':Masked_Root_Mean_Squared_Error_Sigmoid,\n",
    "                                                        'inverse_huber':berHu(0.2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate every Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-88d1062501a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_cloudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloudy_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_cloudy = model.evaluate_generator(cloudy_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_night = model.evaluate_generator(night_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_day = model.evaluate_generator(day_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
