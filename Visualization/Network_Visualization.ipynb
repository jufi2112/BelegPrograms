{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.utils import Sequence # for data generator class\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Activation, BatchNormalization, concatenate, Conv2DTranspose\n",
    "from keras.layers import Add # for skip connections\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "from time import gmtime, strftime\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "class LearningRateDecay:\n",
    "    '''Custom class to reduce learning rate after a specified event (for example after x epochs). \n",
    "    Code is taken from https://www.pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/'''\n",
    "    def plot(self, epochs, title=\"Learning Rate Schedule\"):\n",
    "        lrs = [self(i) for i in epochs]\n",
    "        \n",
    "        N = np.arange(1,len(epochs)+1)\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(N, lrs)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        \n",
    "        \n",
    "class StepDecay(LearningRateDecay):\n",
    "    '''Custom class that implements step decay (i.e. drop learning rate after every x epochs)\n",
    "    Code is taken from https://www.pyimagesearch.com/2019/07/22/keras-learning-rate-schedules-and-decay/'''\n",
    "    def __init__(self, initAlpha=0.01, factor=0.5, dropEvery=10):\n",
    "        self.initAlpha=initAlpha\n",
    "        self.factor=factor\n",
    "        self.dropEvery=dropEvery\n",
    "    \n",
    "    \n",
    "    def __call__(self, epoch):\n",
    "        exp = np.floor((1+epoch) / self.dropEvery)\n",
    "        alpha = self.initAlpha * np.power(self.factor, exp)\n",
    "        return float(alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    '''Assumes that examples in the provided folder are named from 1 to n, with n being the number of images'''\n",
    "    def __init__(self, path_to_data_set='data/train', batch_size=32, image_size=(480,640), shuffle=True, scale_images=False):\n",
    "        self.path_to_data = path_to_data_set\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.scale_images = scale_images\n",
    "        self.training_size = self.__get_training_data_size(self.path_to_data)\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        \n",
    "    def __get_training_data_size(self, path_to_data):\n",
    "        '''gets the number of samples'''\n",
    "        path_color = os.path.join(path_to_data,'Color')\n",
    "        if os.path.isdir(path_color):\n",
    "            size = len([color for color in os.listdir(path_color) if os.path.isfile(os.path.join(path_color, color))])\n",
    "            return size\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''Number of batches per epoche'''\n",
    "        return int(np.floor(self.training_size / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        '''Update indices (and their ordering) after each epoch'''\n",
    "        # image names start with 1, np.arange(n,m) returns values from n to (m-1)\n",
    "        self.indices = np.arange(1, self.training_size+1)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "            \n",
    "    def __data_generation(self, list_images):\n",
    "        '''Generates data of size <batch_size>''' # X = (batch_size, 480, 640, 1)\n",
    "        if self.scale_images == False:\n",
    "            X1 = np.empty((self.batch_size, *self.image_size, 3), dtype=np.uint8) # color images\n",
    "            X2 = np.empty((self.batch_size, *self.image_size), dtype=np.uint16) # ir image\n",
    "        else:\n",
    "            X1 = np.empty((self.batch_size, *self.image_size, 3), dtype=np.float32) # color images\n",
    "            X2 = np.empty((self.batch_size, *self.image_size), dtype=np.float32) # ir image\n",
    "        y = np.empty((self.batch_size, *self.image_size), dtype=np.uint16)  # depth image\n",
    "        # Generate data\n",
    "        for idx, name in enumerate(list_images):\n",
    "            # load images in arrays\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Color', str(name)+\".jpg\"), cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if self.scale_images == False:\n",
    "                X1[idx,] = img.astype(np.uint8)\n",
    "            else:\n",
    "                X1[idx,] = (img/255.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Infrared', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            if self.scale_images == False:\n",
    "                X2[idx,] = img.astype(np.uint16)\n",
    "            else:\n",
    "                X2[idx,] = (img/65535.).astype(np.float32)\n",
    "            img = cv2.imread(os.path.join(self.path_to_data, 'Depth', str(name)+\".png\"), cv2.IMREAD_ANYDEPTH)\n",
    "            y[idx,] = img.astype(np.uint16)\n",
    "        # reshape ir and depth images\n",
    "        X2 = X2.reshape(self.batch_size, 480, 640, 1)\n",
    "        y = y.reshape(self.batch_size, 480, 640, 1)  \n",
    "        return X1, X2, y\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Generate one batch of data, X1 contains 8-bit RGB images, X2 16-bit infrared images and y corresponding 16-bit depth images'''\n",
    "        # Generate indices of data   \n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X1, X2, y = self.__data_generation(indices)\n",
    "        return [X1, X2], y\n",
    "    \n",
    "    \n",
    "def WRONG_Masked_Mean_Absolute_Error(y_true, y_pred):\n",
    "    '''Wrong version of masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.sum(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i, \n",
    "                        axis=(1,2,3)\n",
    "                ) \n",
    "                    /\n",
    "                K.sum(\n",
    "                        A_i,\n",
    "                        axis=(1,2,3)\n",
    "                )\n",
    "           )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Masked_Mean_Absolute_Error(y_true, y_pred):\n",
    "    '''Masked mean absolut error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    loss = K.mean(\n",
    "                K.sum(\n",
    "                        K.abs(y_true - y_pred) * A_i,\n",
    "                        axis=(1,2,3)\n",
    "                     )\n",
    "                /\n",
    "                K.sum(A_i, axis=(1,2,3))\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def Masked_Root_Mean_Squared_Error(y_true, y_pred):\n",
    "    '''Masked root mean squared error custom loss function'''\n",
    "    # create binary artifact maps from ground truth depth maps\n",
    "    A_i = K.greater(y_true, 0)\n",
    "    A_i = K.cast(A_i, dtype='float32')\n",
    "    # original K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
    "    loss = K.sqrt(\n",
    "            K.mean(\n",
    "                    K.sum(\n",
    "                            K.square(y_true - y_pred) * A_i,\n",
    "                            axis=(1,2,3)\n",
    "                         )\n",
    "                    /\n",
    "                    K.sum(A_i, axis=(1,2,3))\n",
    "                  )\n",
    "            )\n",
    "    lower_boundary = K.less(y_pred, 0)\n",
    "    lower_boundary = K.cast(lower_boundary, dtype='float32')\n",
    "    upper_boundary = K.greater(y_pred, 65535)\n",
    "    upper_boundary = K.cast(upper_boundary, dtype='float32')\n",
    "    interval_loss = K.sum(lower_boundary * 10000 + upper_boundary * 10000)   \n",
    "    return loss+interval_loss\n",
    "\n",
    "\n",
    "def berHu(c):\n",
    "    '''Reverse Huber loss as stated in paper \"Deeper Depth Prediction with Fully Convolutional Residual Networks\" by Laina et al. and \"The berhu\n",
    "       penalty and the grouped effect\" by L. Zwald and S. Lambert-Lacroix'''\n",
    "    # does this current implementation makes sense? --> yes, it returns mae or mse\n",
    "    # TODO implement this with binary mask too?\n",
    "    def inverse_huber(y_true, y_pred):\n",
    "        threshold = c * K.max(K.abs(y_true - y_pred))\n",
    "        absolute_mean = K.mean(K.abs(y_true - y_pred))\n",
    "        mask = K.less_equal(absolute_mean, threshold)\n",
    "        mask = K.cast(mask, dtype='float32')\n",
    "        return mask * absolute_mean + (1-mask) * K.mean(K.square(K.abs(y_true - y_pred)))\n",
    "    return inverse_huber\n",
    "        \n",
    "    \n",
    "class VGG:\n",
    "    '''Class that contains building blocks for a residual VGG-like autoencoder network'''\n",
    "    def __init__(self):\n",
    "        self.layer_counting = {}\n",
    "        \n",
    "        \n",
    "    def Block(self, number_of_layers, units, kernel_size, padding, activation, use_bn, momentum_bn):\n",
    "        '''A block of <number_of_layers> convolutions with optional batch normalization added AFTER the non-linearity'''\n",
    "        def Input(z):\n",
    "            for i in range(1,number_of_layers+1):\n",
    "                name = 'Conv' + str(kernel_size[0]) + '-' + str(units)\n",
    "                # make sure we have unique layer names\n",
    "                if name in self.layer_counting:\n",
    "                    self.layer_counting[name] += 1\n",
    "                else:\n",
    "                    self.layer_counting[name] = 1\n",
    "                name += '_' + str(self.layer_counting[name])\n",
    "                z = Conv2D(filters=units, kernel_size=kernel_size, padding=padding, activation=activation, name=name)(z)\n",
    "                if use_bn:\n",
    "                    name_bn = name + '_BN'\n",
    "                    z = BatchNormalization(name=name_bn, momentum=momentum_bn)(z)\n",
    "            return z\n",
    "        return Input\n",
    "    \n",
    "    \n",
    "    def Residual_Downsampling_Block(self, units, kernel_size, padding, activation, use_bn, momentum_bn):\n",
    "        '''A block with a strided convolution for downsampling an the start of a skip connection'''\n",
    "        def Input(z):\n",
    "            skip = z\n",
    "            name = 'DownConv' + str(kernel_size[0]) + '-' + str(units)\n",
    "            # make sure we have unique layer names\n",
    "            if name in self.layer_counting:\n",
    "                self.layer_counting[name] += 1\n",
    "            else:\n",
    "                self.layer_counting[name] = 1\n",
    "            name += '_' + str(self.layer_counting[name])\n",
    "            z = Conv2D(filters=units, kernel_size=kernel_size, strides=(2,2), padding=padding, activation=activation, name=name)(z)\n",
    "            if use_bn:\n",
    "                name_bn = name + '_BN'\n",
    "                z = BatchNormalization(name=name_bn, momentum=momentum_bn)(z)\n",
    "            return z, skip\n",
    "        return Input\n",
    "    \n",
    "    \n",
    "    def Residual_Upsampling_Block(self, units, kernel_size, padding, activation, use_bn, momentum_bn):\n",
    "        '''A block with a transposed convolution (also called deconvolution) and the incorporation of a provided skip connection'''\n",
    "        def Input(z, skip):\n",
    "            name = 'UpConv' + str(kernel_size[0]) + '-' + str(units)\n",
    "            # make sure we have unique layer names\n",
    "            if name in self.layer_counting:\n",
    "                self.layer_counting[name] += 1\n",
    "            else:\n",
    "                self.layer_counting[name] = 1\n",
    "            name += '_' + str(self.layer_counting[name])\n",
    "            name_add = name + '_skip'\n",
    "            z = Conv2DTranspose(filters=units, kernel_size=kernel_size, strides=(2,2), padding=\"same\", name=name)(z)\n",
    "            z = Add(name=name_add)([z, skip])\n",
    "            z = Activation(activation)(z)\n",
    "            if use_bn:\n",
    "                name_bn = name + '_BN'\n",
    "                z = BatchNormalization(name=name_bn, momentum=momentum_bn)(z)\n",
    "            return z\n",
    "        return Input\n",
    "    \n",
    "    \n",
    "    def Residual_Block(self, number_of_layers, units, kernel_size, padding, activation, use_bn, momentum_bn, skip_integration_mode='add'):\n",
    "        '''A block of <number_of_layers> covolutions with provided skip connection incorporated after the last convolutional layer'''\n",
    "        def Input(z, skip):\n",
    "            for i in range(1, number_of_layers+1):\n",
    "                name = 'Conv2D' + str(kernel_size[0]) + '-' + str(units)\n",
    "                # make sure we have unique layer names\n",
    "                if name in self.layer_counting:\n",
    "                    self.layer_counting[name] += 1\n",
    "                else:\n",
    "                    self.layer_counting[name] = 1\n",
    "                name += '_' + str(self.layer_counting[name])\n",
    "                name_add = name + '_skip'\n",
    "                z = Conv2D(filters=units, kernel_size=kernel_size, padding=padding)(z)\n",
    "                if i == number_of_layers:\n",
    "                    if skip_integration_mode.lower() == 'add':\n",
    "                        z = Add(name=name_add)([z, skip])\n",
    "                z = Activation(activation)(z)\n",
    "                if use_bn:\n",
    "                    name_bn = name + '_BN'\n",
    "                    z = BatchNormalization(name=name_bn, momentum=momentum_bn)(z)\n",
    "                if i == number_of_layers:\n",
    "                    if skip_integration_mode.lower() == 'concat':\n",
    "                        z = concatenate([skip, z], name='Concatenate_Skip_0')\n",
    "            return z\n",
    "        return Input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1124 17:31:31.966732 140695125550912 deprecation_wrapper.py:119] From /home/julien/anaconda3/envs/mlenv/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "    #Parser = argparse.ArgumentParser(description=\"Training of a VGG-style autoencoder for depth map prediction\")\n",
    "    #Parser.add_argument(\"-t\", \"--train\", type=str, default=None, help=\"Path to folder that contains the training and validation examples\")\n",
    "    #Parser.add_argument(\"-x\", \"--output\", type=str, default=None, help=\"Path to folder where all output is saved to\")\n",
    "    #Parser.add_argument(\"-b\", \"--batch_size\", type=int, default=4, help=\"Batch size to train the network with\")\n",
    "    #Parser.add_argument(\"-e\", \"--epochs\", type=int, default=30, help=\"Number of epochs to train the network on\")\n",
    "    #Parser.add_argument(\"--no_shuffle\", default=False, action='store_true', help=\"Disables shuffling of batches for each epoch\")\n",
    "    #Parser.add_argument(\"--no_scale\", default=False, action='store_true', help=\"Disables scaling of input images to the range of [0,1]\")\n",
    "    #Parser.add_argument(\"-o\", \"--optimizer\", type=str, default=\"rmsprop\", help=\"The optimizer to utilize for training. Supported are SGD, Adam and RMSprop.\")\n",
    "    #Parser.add_argument(\"-p\", \"--periods\", type=int, default=1, help=\"Number of epochs after which to save the current model (and its weights). 1 means every epoch.\")\n",
    "    #Parser.add_argument(\"-d\", \"--decay\", type=int, default=10, help=\"Reduce learning rate after every x epochs. Defaults to 10\")\n",
    "    #Parser.add_argument(\"-f\", \"--factor_decay\", type=float, default=0.5, help=\"Factor to reduce the learning rate. Defaults to 0.5\")\n",
    "    #Parser.add_argument(\"--default_optimizers\", default=False, action='store_true', help=\"Enable all keras optimizers, not only SGD, Adam and RMSprop. This will deactivate learning rate decay.\")\n",
    "    #Parser.add_argument(\"--omit_batchnorm\", default=False, action='store_true', help=\"Don't add batch normalization layers after convolutions.\")\n",
    "    #Parser.add_argument(\"-m\", \"--momentum\", type=float, default=0.99, help=\"Momentum used in batch normalization layers. Defaults to 0.99. If validation loss oscillates, try lowering it (e.g. to 0.6)\")\n",
    "    #Parser.add_argument(\"--skip_0\", type=str, default=\"add\", help=\"Functionality of S0 skip connections. One of the following: 'add', 'concat', 'concat+' or 'disable'. Defaults to 'add'. 'Concat+' adds convolutions after concatenating.\")\n",
    "    #Parser.add_argument(\"--sgd_momentum\", type=str, default=None, help=\"Only works when using SGD optimizer: Not specified/'None': no momentum, 'normal': momentum with value from --sgd_momentum_value, 'nesterov': Use nesterov momentum with value from --sgd_momentum_value.\")\n",
    "    #Parser.add_argument(\"--sgd_momentum_value\", type=float, default=0.9, help=\"Only works when using SGD optimizer: Momentum value for SGD optimizer. Enable by using --sgd_momentum. Defaults to 0.9\")\n",
    "    #Parser.add_argument(\"-l\", \"--loss\", type=str, default=\"MMAE\", help=\"Loss function to utilize. Either MMAE or MRMSE. Defaults to MMAE\")\n",
    "    #args = Parser.parse_args()\n",
    "    \n",
    "    # training directory specified?\n",
    "    #if args.train is None:\n",
    "        #print(\"No directory with training examples specified!\")\n",
    "        #print(\"For help use --help\")\n",
    "        #exit()\n",
    "        \n",
    "    # does train folder exists?\n",
    "    #if not os.path.isdir(os.path.join(args.train, 'train')):\n",
    "        #print(\"Provided training directory contains no subfolder 'train'\")\n",
    "        #exit()\n",
    "        \n",
    "    # valid batch size?\n",
    "    #if args.batch_size <= 0:\n",
    "        #print(\"Invalid batch size supplied!\")\n",
    "        #exit()\n",
    "        \n",
    "    # valid epochs?\n",
    "    #if args.epochs <= 0:\n",
    "        #print(\"Invalid number of epochs supplied!\")\n",
    "        #exit()\n",
    "        \n",
    "        \n",
    "    # output directory specified?\n",
    "    #if args.output is None:\n",
    "        #print(\"No output directory specified!\")\n",
    "        #print(\"For help use --help\")\n",
    "        #exit()\n",
    "        \n",
    "        \n",
    "    #loss_func = None\n",
    "    #loss = args.loss.lower()\n",
    "    #if loss == \"mrmse\":\n",
    "    #    print(\"Using masked-root-mean-squared-error loss function\")\n",
    "    #    loss_func = Masked_Root_Mean_Squared_Error\n",
    "    #elif loss == \"mmae\":\n",
    "    #    print(\"Using masked-mean-absolute-error loss function\")\n",
    "    #    loss_func = Masked_Mean_Absolute_Error\n",
    "    #else:\n",
    "    #    print(\"Provided loss function is invalid. Defaulting to MMAE\")\n",
    "    #    loss_func = Masked_Mean_Absolute_Error\n",
    "        \n",
    "    # skip connection 0 arguments\n",
    "    #s0_arg = args.skip_0.lower()\n",
    "    #if s0_arg != 'add' and s0_arg != 'concat' and s0_arg != 'disable' and s0_arg != 'concat+':\n",
    "    #    print(\"Invalid argument for '--skip_0': \" + s0_arg)\n",
    "    #    print(\"Defaulting to 'add'\")\n",
    "    #    s0_arg = 'add'\n",
    "        \n",
    "    # create output directory\n",
    "    #os.makedirs(args.output, exist_ok=True)\n",
    "    # create folder for logs\n",
    "    #log_dir = os.path.join(args.output, 'logs')\n",
    "    #os.makedirs(log_dir, exist_ok=True)\n",
    "    # create folder for intermediate models\n",
    "    #model_dir = os.path.join(args.output, 'models')\n",
    "    #os.makedirs(model_dir, exist_ok=True)\n",
    "    # create folder for figures\n",
    "    #figure_dir = os.path.join(args.output, 'figures')\n",
    "    #os.makedirs(figure_dir, exist_ok=True)        \n",
    "        \n",
    "    #schedule = None\n",
    "    #optimizer = None\n",
    "    #if not args.default_optimizers:\n",
    "    #    if args.optimizer.lower() == 'adam':\n",
    "    #        print(\"Using adam optimizer\")\n",
    "    #        optimizer = Adam(lr=0.001)\n",
    "    #        schedule = StepDecay(initAlpha=0.001, factor=args.factor_decay, dropEvery=args.decay)\n",
    "    #    \n",
    "    #    elif args.optimizer.lower() == 'rmsprop':\n",
    "    #        print(\"Using RMSprop optimizer\")\n",
    "    #        optimizer = RMSprop(lr=0.001)\n",
    "    #        schedule = StepDecay(initAlpha=0.001, factor=args.factor_decay, dropEvery=args.decay)\n",
    "    #    \n",
    "    #    elif args.optimizer.lower() == 'sgd':\n",
    "    #        # check for momentum:\n",
    "    #        if args.sgd_momentum is None:\n",
    "    #            print(\"Using SGD optimizer without momentum\")\n",
    "    #            optimizer = SGD(lr=0.01)\n",
    "    #        else:\n",
    "    #            if args.sgd_momentum.lower() == 'normal':\n",
    "    #                print(\"Using normal SGD momentum = \" + str(args.sgd_momentum_value))\n",
    "    #                optimizer = SGD(lr=0.01, momentum=args.sgd_momentum_value, nesterov=False)\n",
    "    #            elif args.sgd_momentum.lower() == 'nesterov':\n",
    "    #                print(\"Using nesterov SGD momentum = \" + str(args.sgd_momentum_value))\n",
    "    #                optimizer = SGD(lr=0.01, momentum=args.sgd_momentum_value, nesterov=True)\n",
    "    #            else:\n",
    "    #               print(\"Unknown --sgd_momentum value. Defaulting to None\")\n",
    "    #                optimizer = SGD(lr=0.01)\n",
    "    #        schedule = StepDecay(initAlpha=0.01, factor=args.factor_decay, dropEvery=args.decay)\n",
    "\n",
    "    #    if optimizer is None:\n",
    "    #        print(\"Unsupported optimizer provided. If you want to use this unsupported optimizer, provide --custom False\")\n",
    "    #        print(\"For help use --help\")\n",
    "    #        exit()\n",
    "    #else:\n",
    "    #   optimizer = args.optimizer\n",
    "        \n",
    "training_generator = DataGenerator(\n",
    "            path_to_data_set=os.path.join('data', 'train'),\n",
    "            batch_size=4,\n",
    "            image_size=(480,640),\n",
    "            shuffle=True,\n",
    "            scale_images=True\n",
    "        )\n",
    "\n",
    "validation_generator = DataGenerator(\n",
    "                path_to_data_set=os.path.join('data', 'validation'),\n",
    "                batch_size=4,\n",
    "                image_size=(480,640),\n",
    "                shuffle=True,\n",
    "                scale_images=True\n",
    "            )\n",
    "\n",
    "    #checkpoint_callback = ModelCheckpoint(\n",
    "    #        filepath=checkpoint_path,\n",
    "    #        verbose=1,\n",
    "    #        save_best_only=False,\n",
    "    #        save_weights_only=False,\n",
    "    #        mode='auto',\n",
    "    #        period=args.periods)\n",
    "\n",
    "    #tensorboard_callback = TensorBoard(\n",
    "    #        log_dir=log_path,\n",
    "    #        histogram_freq = 0,\n",
    "    #        batch_size=args.batch_size,\n",
    "    #        write_graph=True,\n",
    "    #        write_grads=False,\n",
    "    #        write_images=True,\n",
    "    #        update_freq=\"epoch\")\n",
    "\n",
    "    #callback_list = [checkpoint_callback, tensorboard_callback]\n",
    "    #callback_list = [tensorboard_callback]\n",
    "    #if schedule is not None:\n",
    "    #    callback_list.append(LearningRateScheduler(schedule))\n",
    "omit_batchnorm = False\n",
    "batchnorm_momentum = 0.99\n",
    "s0_arg='add'\n",
    "    \n",
    "vgg = VGG()\n",
    "# Color branch\n",
    "input_color = Input(shape=(480,640,3), name=\"Color_Input\")\n",
    "x = Model(inputs=input_color, outputs=input_color)\n",
    "    \n",
    "# Infrared branch\n",
    "input_ir = Input(shape=(480,640,1), name=\"Infrared_Input\")\n",
    "y = Model(inputs=input_ir, outputs=input_ir)\n",
    "\n",
    "# combine both branches\n",
    "combined = concatenate([x.output, y.output], name=\"Concatenate_Input\")\n",
    "\n",
    "# zeroth skip connection start --> to transfer original input images to the end of the network\n",
    "skip_zero = combined\n",
    "\n",
    "# VGG16 style encoder (configuration D)\n",
    "\n",
    "z = vgg.Block(number_of_layers=2, units=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(combined)\n",
    "# max pooling replaced with strided convolution + first skip connection start\n",
    "z, skip_one = vgg.Residual_Downsampling_Block(units=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "z = vgg.Block(number_of_layers=2, units=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "# max pooling replaced with strided convolution + second skip connection start\n",
    "z, skip_two = vgg.Residual_Downsampling_Block(units=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "z = vgg.Block(number_of_layers=3, units=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "# max pooling replaced with strided convolution + third skip connection start\n",
    "z, skip_three = vgg.Residual_Downsampling_Block(units=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "z = vgg.Block(number_of_layers=3, units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "# max pooling replaced with strided convolution + fourth skip connection start\n",
    "z, skip_four = vgg.Residual_Downsampling_Block(units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "z = vgg.Block(number_of_layers=3, units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "# max pooling replaced with strided convolution + fifth skip connection start\n",
    "z, skip_five = vgg.Residual_Downsampling_Block(units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "# end of encoder part\n",
    "\n",
    "z = vgg.Block(number_of_layers=3, units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "# start of decoder part (= mirrored encoder part)\n",
    "\n",
    "# upsampling with deconvolution + fifth skip connection target\n",
    "z = vgg.Residual_Upsampling_Block(units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z, skip_five)\n",
    "z = vgg.Block(number_of_layers=3, units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "# upsampling with deconvolution + fourth skip connection target\n",
    "z = vgg.Residual_Upsampling_Block(units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z, skip_four)\n",
    "z = vgg.Block(number_of_layers=3, units=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "# upsampling with deconvolution + third skip connection target\n",
    "z = vgg.Residual_Upsampling_Block(units=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z, skip_three)\n",
    "z = vgg.Block(number_of_layers=3, units=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "# upsampling with deconvolution + second skip connection target\n",
    "z = vgg.Residual_Upsampling_Block(units=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z, skip_two)\n",
    "z = vgg.Block(number_of_layers=2, units=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "\n",
    "    # upsampling with deconvolution + first skip connection target\n",
    "z = vgg.Residual_Upsampling_Block(units=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z, skip_one)\n",
    "z = vgg.Block(number_of_layers=2, units=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "    # end of decoder part\n",
    "    \n",
    "if s0_arg == 'add':\n",
    "    z = vgg.Residual_Block(number_of_layers=1, units=4, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum, skip_integration_mode='add')(z, skip_zero)\n",
    "elif s0_arg == 'concat' or s0_arg == 'concat+':\n",
    "    z = vgg.Residual_Block(number_of_layers=1, units=4, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum, skip_integration_mode='concat')(z, skip_zero)\n",
    "    if s0_arg == 'concat+':\n",
    "        z = vgg.Block(number_of_layers=2, units=4, kernel_size=(3,3), padding=\"same\", activation=\"relu\", use_bn=not omit_batchnorm, momentum_bn=batchnorm_momentum)(z)\n",
    "\n",
    "    # output layer\n",
    "z = Conv2D(1, kernel_size=(3,3), padding=\"same\", name=\"Conv_Output\")(z)\n",
    "\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "model.compile(\n",
    "            optimizer='sgd',\n",
    "            loss=Masked_Mean_Absolute_Error,\n",
    "            metrics=['mae', 'mse', Masked_Mean_Absolute_Error, Masked_Root_Mean_Squared_Error, \"accuracy\", berHu(0.2)])\n",
    "    \n",
    "    #hist = model.fit_generator(\n",
    "    #       generator=training_generator,\n",
    "    #       validation_data=validation_generator,\n",
    "    #       epochs=100,\n",
    "    #       callbacks=None)\n",
    "    \n",
    "    # plot the learning rate and loss\n",
    "    #N = np.arange(1, args.epochs+1)\n",
    "    #plt.style.use(\"ggplot\")\n",
    "    #plt.figure()\n",
    "    #plt.plot(N, hist.history[\"loss\"], label=\"train_loss\")\n",
    "    #plt.plot(N, hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "    #plt.title(\"Training Loss with Optimizer: \" + args.optimizer)\n",
    "    #plt.xlabel(\"Epochs\")\n",
    "    #plt.ylabel(\"Loss\")\n",
    "    #plt.legend()\n",
    "    #plt.savefig(os.path.join(figure_dir,args.optimizer + '_' + str(args.epochs) + '_metrics'))\n",
    "    \n",
    "    #if schedule is not None:\n",
    "    #    N = np.arange(0, args.epochs)\n",
    "    #    schedule.plot(N)\n",
    "    #    plt.savefig(os.path.join(figure_dir,args.optimizer + '_' + str(args.epochs) + '_lr'))\n",
    "    \n",
    "    \n",
    "    #pickle.dump(hist.history, open(os.path.join(args.output,'history.p'), 'wb'))\n",
    "    \n",
    "    #model.save(os.path.join(args.output,'final_model.h5'))\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
